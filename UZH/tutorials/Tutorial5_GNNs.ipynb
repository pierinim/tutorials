{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0d13aff9",
      "metadata": {
        "id": "0d13aff9"
      },
      "source": [
        "# GNN Tutorial (PyG) â€” Exercise\n",
        "\n",
        "Dataset citation:\n",
        "\n",
        "https://chrsmrrs.github.io/datasets/docs/home/\n",
        "Riesen, K. and Bunke, H.: IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning. In: da Vitora Lobo, N. et al. (Eds.), SSPR&SPR 2008, LNCS, vol. 5342, pp. 287-297, 2008.\n",
        "\n",
        "AIDS Antiviral Screen Data (2004)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4476ef9c",
      "metadata": {
        "id": "4476ef9c"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-geometric"
      ],
      "metadata": {
        "id": "7Fvnrmoyu33m"
      },
      "id": "7Fvnrmoyu33m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80630f81",
      "metadata": {
        "id": "80630f81"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.typing import Adj\n",
        "\n",
        "print('PyTorch:', torch.__version__)\n",
        "print('PyG:', torch_geometric.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1aba0e0",
      "metadata": {
        "id": "c1aba0e0"
      },
      "source": [
        "## NetworkX Star Graph (warm-up)\n",
        "\n",
        "Exercise: create a star graph with 1 center and 3 leaves, and plot it with labels.\n",
        "Hint: you can use `nx.star_graph(n)` or add nodes and edges manually (DiGraph).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e13c42",
      "metadata": {
        "id": "90e13c42"
      },
      "outputs": [],
      "source": [
        "# TODO: Create and plot a directed star graph with 1 center and 3 leaves\n",
        "# and then plot it.\n",
        "\n",
        "raise RuntimeError('Checkpoint - Star Graph')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58155acd",
      "metadata": {
        "id": "58155acd"
      },
      "source": [
        "## Download dataset (if missing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "069da409",
      "metadata": {
        "id": "069da409"
      },
      "outputs": [],
      "source": [
        "# Colab/Local: fetch graph_db.bin from Google Drive if not present\n",
        "if not os.path.exists('graph_db.bin'):\n",
        "    print('Downloading graph_db.bin ...')\n",
        "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1IHy7C6uZnAQucvW3jI-_fs6vAQfcn3HQ' -O graph_db.bin\n",
        "else:\n",
        "    print('graph_db.bin found, skipping download.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25197fc1",
      "metadata": {
        "id": "25197fc1"
      },
      "source": [
        "## Utils: one-hot and conversion NetworkX -> PyG Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d29ffe3",
      "metadata": {
        "id": "3d29ffe3"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(label: int, num_classes: int):\n",
        "    vec = np.zeros(num_classes, dtype=np.float32)\n",
        "    vec[int(label)] = 1.0\n",
        "    return vec.tolist()\n",
        "\n",
        "def _as_class_index(val, num_classes):\n",
        "    arr = np.asarray(val)\n",
        "    if arr.ndim == 0:\n",
        "        return int(arr)\n",
        "    arr = arr.reshape(-1)\n",
        "    if arr.size == 1:\n",
        "        return int(arr[0])\n",
        "    return int(np.argmax(arr))\n",
        "\n",
        "def convert_nx_to_pyg_data(G: nx.Graph, num_node_label_classes: int = 38, num_edge_label_classes: int = 3) -> Data:\n",
        "    node_ids_sorted = sorted(list(G.nodes()))\n",
        "    nid_to_idx = {nid: i for i, nid in enumerate(node_ids_sorted)}\n",
        "    node_features = []\n",
        "    for nid in node_ids_sorted:\n",
        "        attrs = np.asarray(G.nodes[nid]['attributes'], dtype=np.float32).tolist()\n",
        "        nlab = _as_class_index(G.nodes[nid]['labels'], num_node_label_classes)\n",
        "        feat = np.asarray(list(attrs) + to_one_hot(nlab, num_node_label_classes), dtype=np.float32)\n",
        "        node_features.append(feat)\n",
        "    x = torch.tensor(np.stack(node_features, axis=0), dtype=torch.float32)\n",
        "    src, dst, eattr = [], [], []\n",
        "    for (u, v) in G.edges():\n",
        "        elab_raw = G.edges[(u, v)]['labels']\n",
        "        arr = np.asarray(elab_raw).reshape(-1)\n",
        "        if arr.size == num_edge_label_classes:\n",
        "            one = arr.astype(np.float32)\n",
        "        else:\n",
        "            one = np.asarray(to_one_hot(_as_class_index(elab_raw, num_edge_label_classes), num_edge_label_classes), dtype=np.float32)\n",
        "        ui = nid_to_idx[u]; vi = nid_to_idx[v]\n",
        "        src += [ui, vi]; dst += [vi, ui]; eattr += [one, one]\n",
        "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
        "    edge_attr = torch.tensor(np.stack(eattr, axis=0), dtype=torch.float32)\n",
        "    y_scalar = float(_as_class_index(G.graph.get('classes', 0), 2))\n",
        "    y = torch.tensor([y_scalar], dtype=torch.float32)\n",
        "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "954c990a",
      "metadata": {
        "id": "954c990a"
      },
      "source": [
        "## Load and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9abd1889",
      "metadata": {
        "id": "9abd1889"
      },
      "outputs": [],
      "source": [
        "def load_graphs_pickle(path: str, limit: int = -1) -> List[nx.Graph]:\n",
        "    db = pickle.load(open(path, 'rb'))\n",
        "    if limit is not None and limit > 0:\n",
        "        db = db[:limit]\n",
        "    return db\n",
        "\n",
        "def nx_list_to_pyg(db: List[nx.Graph]) -> List[Data]:\n",
        "    return [convert_nx_to_pyg_data(G) for G in tqdm(db, desc='Converting to PyG')]\n",
        "\n",
        "graphs_nx = load_graphs_pickle('graph_db.bin', limit=-1)\n",
        "data_list = nx_list_to_pyg(graphs_nx)\n",
        "len(data_list), data_list[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e534311",
      "metadata": {
        "id": "5e534311"
      },
      "source": [
        "### Plot two samples with NetworkX (active vs inactive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d0af911",
      "metadata": {
        "id": "2d0af911"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_networkx_graph(G: nx.Graph, ax=None, title: str = 'Molecule'):\n",
        "    numeric_to_str_atomic_codes = { 0: 'C',1: 'O',2: 'N',3: 'Cl',4: 'F',5: 'S',6: 'Se',7: 'P',8: 'Na',9: 'I',\n",
        "                                    10: 'Co',11: 'Br',12: 'Li',13: 'Si',14: 'Mg',15: 'Cu',16: 'As',17: 'B',\n",
        "                                    18: 'Pt',19: 'Ru',20: 'K',21: 'Pd',22: 'Au',23: 'Te',24: 'W',25: 'Rh',26: 'Zn',\n",
        "                                    27: 'Bi',28: 'Pb',29: 'Ge',30: 'Sb',31: 'Sn',32: 'Ga',33: 'Hg',34: 'Ho',35: 'Tl',\n",
        "                                    36: 'Ni',37: 'Tb'}\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    pos = nx.spring_layout(G, seed=0)\n",
        "    nx.draw(G, pos, with_labels=False, node_color='lightblue', node_size=450, ax=ax)\n",
        "    attrs = nx.get_node_attributes(G, 'attributes')\n",
        "    if attrs:\n",
        "        node_label_atom = {k: numeric_to_str_atomic_codes.get(int(v[0]), str(int(v[0]))) for k, v in attrs.items()}\n",
        "        nx.draw_networkx_labels(G, pos, labels=node_label_atom, font_size=10, ax=ax)\n",
        "    elabs = nx.get_edge_attributes(G, 'labels')\n",
        "    if elabs:\n",
        "        nx.draw_networkx_edge_labels(G, pos, edge_labels={(i,j): str(elabs[(i,j)]) for (i,j) in G.edges()}, ax=ax)\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "\n",
        "# Helper to get binary class from graph meta (supports int or one-hot)\n",
        "import numpy as _np\n",
        "\n",
        "def get_label(G: nx.Graph) -> int:\n",
        "    y = G.graph.get('classes', 0)\n",
        "    arr = _np.asarray(y)\n",
        "    if arr.ndim == 0:\n",
        "        return int(arr)\n",
        "    arr = arr.reshape(-1)\n",
        "    if arr.size == 1:\n",
        "        return int(arr[0])\n",
        "    return int(arr.argmax())\n",
        "\n",
        "# Pick one active (1) and one inactive (0) molecule\n",
        "active = next((g for g in graphs_nx if get_label(g) == 1), graphs_nx[0])\n",
        "inactive = next((g for g in graphs_nx if get_label(g) == 0), graphs_nx[-1])\n",
        "\n",
        "# Plot side-by-side using subplots\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5), dpi=140)\n",
        "plot_networkx_graph(active, ax=axs[0], title='Active (label=1)')\n",
        "plot_networkx_graph(inactive, ax=axs[1], title='Inactive (label=0)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ea07406",
      "metadata": {
        "id": "0ea07406"
      },
      "source": [
        "## Train/Val/Test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2284b066",
      "metadata": {
        "id": "2284b066"
      },
      "outputs": [],
      "source": [
        "def split_data(data_list: List[Data], seed: int = 42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(len(data_list))\n",
        "    rng.shuffle(idx)\n",
        "    n = len(idx)\n",
        "    n_val = int(0.2 * n); n_test = int(0.2 * n)\n",
        "    val_idx = idx[:n_val]; test_idx = idx[n_val:n_val + n_test]; train_idx = idx[n_val + n_test:]\n",
        "    take = lambda ids: [data_list[i] for i in ids]\n",
        "    return take(train_idx), take(val_idx), take(test_idx)\n",
        "\n",
        "train_data, val_data, test_data = split_data(data_list, seed=42)\n",
        "len(train_data), len(val_data), len(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0231da31",
      "metadata": {
        "id": "0231da31"
      },
      "source": [
        "## DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5ea0a42",
      "metadata": {
        "id": "c5ea0a42"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "len(train_loader), len(val_loader), len(test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bb58fa",
      "metadata": {
        "id": "f0bb58fa"
      },
      "source": [
        "## PyTorch Primer: Simple CNN\n",
        "A compact CNN example to illustrate core PyTorch patterns (modules, forward, tensors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed3ca2e",
      "metadata": {
        "id": "9ed3ca2e"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, in_channels: int = 1, num_classes: int = 10):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "        ])\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.head = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for conv in self.convs:\n",
        "            x = F.relu(conv(x))\n",
        "            x = self.pool(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "# Example forward pass on dummy data (MNIST-like)\n",
        "cnn = SimpleCNN(in_channels=1, num_classes=10)\n",
        "demo_x = torch.randn(4, 1, 28, 28)\n",
        "demo_logits = cnn(demo_x)\n",
        "print('Output shape:', demo_logits.shape)\n",
        "\n",
        "raise RuntimeError('Checkpoint - Simple CNN')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4f888df",
      "metadata": {
        "id": "f4f888df"
      },
      "source": [
        "## Model (MPNN-style)\n",
        "We now implement a message passing layer and a small GNN.\n",
        "PyG `Data` packs graph tensors: `x` (node features), `edge_index` (2Ã—E indices), `edge_attr` (edge features), and `batch` (graph id per node).\n",
        "The line `x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch` unpacks those for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a86cc929",
      "metadata": {
        "id": "a86cc929"
      },
      "outputs": [],
      "source": [
        "class MPNNLayer(MessagePassing):\n",
        "    \"\"\"\n",
        "    A single message passing layer following the MPNN formalism:\n",
        "      - message:   m_v = sum_{w in N(v)} M_t(h_w, h_v, e_{wv})\n",
        "      - update:    h_v' = U_t(h_v, m_v)\n",
        "\n",
        "    Where h_v, h_w are node states and e_{wv} is the edge feature.\n",
        "\n",
        "    Your task: implement message() and update() with small MLPs or linear+nonlinearity.\n",
        "    Tip: you will likely concatenate tensors along the last dim.\n",
        "    Shapes (per edge or node, at runtime):\n",
        "      h_v: [hidden], h_w: [hidden], e_{wv}: [edge_dim]\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden: int, edge_dim: int, dropout: float = 0.0, aggr: str = 'add'):\n",
        "        super().__init__(aggr=aggr)\n",
        "        self.dropout = dropout\n",
        "        # Example layer you might create:\n",
        "        # self.m1 = nn.Linear(hidden + edge_dim + hidden, hidden)\n",
        "        # Note: In PyTorch, activations are separate â€” call F.relu(tensor) explicitly\n",
        "        # between Linear layers (or use nn.ReLU modules).\n",
        "\n",
        "    def forward(self, h: torch.Tensor, edge_index: Adj, e_wv: torch.Tensor):\n",
        "        \"\"\"\n",
        "        h: node states (a.k.a. x)\n",
        "        edge_index: [2, E] directed edges (source->target indices)\n",
        "        e_wv: edge features per edge\n",
        "        \"\"\"\n",
        "        return self.propagate(edge_index, h=h, e_wv=e_wv)\n",
        "\n",
        "    def message(self, h_i: torch.Tensor, h_j: torch.Tensor, e_wv: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Message function M_t(h_w, h_v, e_{wv}) computed on each edge (w->v):\n",
        "        - h_j corresponds to h_w (source node on edge)\n",
        "        - h_i corresponds to h_v (target node on edge)\n",
        "        - edge_attr corresponds to e_{wv}\n",
        "        Return one message vector per edge with shape [hidden].\n",
        "\n",
        "        Note: Concatenate tensors with torch.cat([t1, t2, t3], dim=-1).\n",
        "        \"\"\"\n",
        "\n",
        "        # Aliases to match the notation we studied in the last lecture\n",
        "        h_w = h_j\n",
        "        h_v = h_i\n",
        "\n",
        "\n",
        "        m = None  # TODO: implement message\n",
        "        return m\n",
        "\n",
        "    def update(self, aggr_out: torch.Tensor, h: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Update function U_t(h_v, m_v) computed on each node v after messages are aggregated:\n",
        "        - aggr_out is the aggregated message m_v (sum/mean over incoming edges)\n",
        "        - x is the previous node state h_v\n",
        "        Return new node state h_v' with shape [hidden].\n",
        "        \"\"\"\n",
        "        # Aliases to match the notation from the paper\n",
        "        m_v = aggr_out\n",
        "        h_v = h\n",
        "        h = None  # TODO: implement update U([h_v=h, m_v=aggr_out])\n",
        "        return h\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Build a full graph network using several MPNNLayer blocks.\n",
        "    Outline:\n",
        "      - Project input node features (in_dim) to hidden via a Linear layer.\n",
        "      - Stack k MPNNLayer blocks (ModuleList) and apply non-linearities/dropout between them.\n",
        "      - Pool node embeddings to a single graph embedding using a permutation-invariant readout.\n",
        "        global_mean_pool averages node embeddings per graph id â€” useful for graph-level tasks.\n",
        "      - Use a small head to produce a single logit per graph.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dropout = 0.5\n",
        "        self.edge_dim = 3\n",
        "        self.input_node_dim = 42\n",
        "        self.lin_in = nn.Linear(self.input_node_dim, 64)\n",
        "        # TODO: build the full GNN:\n",
        "        #  - create a stack of MPNNLayer: self.layers = nn.ModuleList([MPNNLayer(64, 3, dropout=0.5) for _ in range(3)])\n",
        "        #  - define a small readout head: e.g., self.out1 = nn.Linear(64, 64); self.out2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, data: Data):\n",
        "        # PyG Data packs graph tensors (batched):\n",
        "        #  x: [total_nodes, in_dim]          node features\n",
        "        #  edge_index: [2, E]                 source/target indices for each edge\n",
        "        #  edge_attr: [E, edge_dim]           edge (bond) features\n",
        "        #  batch: [total_nodes]               graph id for each node (to pool by graph)\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "        # 1) Input projection to hidden dim\n",
        "        x = self.lin_in(x)\n",
        "\n",
        "        # 2) Message passing stack\n",
        "        x = None  # TODO: iterate your MPNNLayer stack over (x, edge_index, edge_attr); add ReLU/Dropout between layers\n",
        "\n",
        "        # 3) Graph readout:\n",
        "        # global_mean_pool(x, batch) aggregates node embeddings into one vector per graph by averaging.\n",
        "        # After pooling, add a small MLP/Linear head to get a single logit per graph.\n",
        "\n",
        "        return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GNN().to(device)\n",
        "model\n",
        "\n",
        "raise RuntimeError('Checkpoint - Build the GNN (implement message/update, layer stack, pooling, and head)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e9d3346",
      "metadata": {
        "id": "8e9d3346"
      },
      "outputs": [],
      "source": [
        "def bce_logits_loss():\n",
        "    return nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Standard training epoch\n",
        "def train_one_epoch(model, loader, opt, criterion):\n",
        "    model.train()\n",
        "    total_loss, total_acc, total_count = 0.0, 0.0, 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(batch)  # [B]\n",
        "        y = batch.y.view(-1).to(device)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        # metrics\n",
        "        with torch.no_grad():\n",
        "            preds = (logits.sigmoid() > 0.5).float()\n",
        "            yb = y.view_as(preds)\n",
        "            acc = (preds == yb).float().mean().item()\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        total_acc += acc * y.size(0)\n",
        "        total_count += y.size(0)\n",
        "    return total_loss / max(total_count, 1), total_acc / max(total_count, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, total_count = 0.0, 0.0, 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        logits = model(batch)\n",
        "        y = batch.y.view(-1).to(device)\n",
        "        loss = criterion(logits, y)\n",
        "        preds = (logits.sigmoid() > 0.5).float()\n",
        "        yb = y.view_as(preds)\n",
        "        acc = (preds == yb).float().mean().item()\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        total_acc += acc * y.size(0)\n",
        "        total_count += y.size(0)\n",
        "    return total_loss / max(total_count, 1), total_acc / max(total_count, 1)\n",
        "\n",
        "# Training config (run after you finish the GNN implementation)\n",
        "epochs = 20\n",
        "lr = 1e-4\n",
        "criterion = bce_logits_loss()\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Track metrics for plotting\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "for ep in range(1, epochs + 1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    va_loss, va_acc = evaluate(model, val_loader, criterion)\n",
        "    history['train_loss'].append(tr_loss)\n",
        "    history['train_acc'].append(tr_acc)\n",
        "    history['val_loss'].append(va_loss)\n",
        "    history['val_acc'].append(va_acc)\n",
        "    print(f'Epoch {ep:02d} | train loss {tr_loss:.4f} acc {tr_acc:.3f} | val loss {va_loss:.4f} acc {va_acc:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e0d10cb",
      "metadata": {
        "id": "9e0d10cb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-v0_8')\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4), dpi=140)\n",
        "epochs_range = range(1, epochs + 1)\n",
        "# Loss\n",
        "axs[0].plot(epochs_range, history['train_loss'], label='Train Loss', color='#1f77b4', marker='o', linewidth=2, markersize=3)\n",
        "axs[0].plot(epochs_range, history['val_loss'], label='Val Loss', color='#ff7f0e', marker='o', linewidth=2, markersize=3)\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].set_title('Loss over Epochs')\n",
        "axs[0].grid(True, alpha=0.3)\n",
        "axs[0].legend(frameon=False)\n",
        "# Accuracy\n",
        "axs[1].plot(epochs_range, history['train_acc'], label='Train Acc', color='#2ca02c', marker='o', linewidth=2, markersize=3)\n",
        "axs[1].plot(epochs_range, history['val_acc'], label='Val Acc', color='#d62728', marker='o', linewidth=2, markersize=3)\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].set_ylim(0.0, 1.0)\n",
        "axs[1].set_title('Accuracy over Epochs')\n",
        "axs[1].grid(True, alpha=0.3)\n",
        "axs[1].legend(frameon=False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b81824e",
      "metadata": {
        "id": "9b81824e"
      },
      "outputs": [],
      "source": [
        "te_loss, te_acc = evaluate(model, test_loader, criterion)\n",
        "print(f'Test loss {te_loss:.4f} acc {te_acc:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture\n",
        "https://drive.google.com/drive/folders/1sBIx19EuHKTkuLEu5iv1DWOPa3Zvtn3c?usp=share_link\n",
        "\n",
        "\n",
        "## Solution\n",
        "https://colab.research.google.com/drive/1yIB5VGS_BTPyTzjzSTlDK2ozgElBlXpd?usp=sharing\n",
        "\n"
      ],
      "metadata": {
        "id": "q0DTVYpyPe1n"
      },
      "id": "q0DTVYpyPe1n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}